[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the website!",
    "section": "",
    "text": "On the site you can see my portfolio, as well as visit the blog\nAbout be click.\nDon‚Äôt be shy about contacting me on the links below!\n\n \n  \n   \n  \n    \n     Telegram\n  \n  \n    \n     WhatsApp\n  \n  \n    \n     Email"
  },
  {
    "objectID": "index.html#portfolio",
    "href": "index.html#portfolio",
    "title": "Welcome to the website!",
    "section": "Portfolio",
    "text": "Portfolio\nFollow the link to view publications in the portfolio\n\n\n\n\n\n\n\n\nBinol vs Olive\n\n\n\n14 Dec 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOCR Reader\n\n\n\n06 Dec 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping\n\n\n\n20 Jul 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBeautiful export to Excel (xlsx)\n\n\n\n10 Jul 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCheese sales\n\n\n\n06 Jul 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRevenue segment\n\n\n\n03 Jul 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/portfolio/posts/web-scraping.html",
    "href": "content/portfolio/posts/web-scraping.html",
    "title": "Web Scraping",
    "section": "",
    "text": "This code allows you to parse information about vacancies on hh.ru It scans the home page, finds vacancies, follows links to vacancies and collects the specified data\n\nImport and settings\nThe main libraries for parsing are: 1. requests - makes a request to the site and receives data 2. bs4 (BeautifulSoup) - parses the result of the request requests\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport polars as pl\nimport re\nimport xlsxwriter\nimport time\n\n1df = pl.DataFrame({\n    \"URL\": pl.Series([], dtype=pl.Utf8),\n    \"–í–∞–∫–∞–Ω—Å–∏—è\": pl.Series([], dtype=pl.Utf8),\n    \"–ó–∞—Ä–ø–ª–∞—Ç–∞\": pl.Series([], dtype=pl.Int64)\n    \"Keyword\": pl.Series([], dtype=pl.Utf8),\n    \"Tags\": pl.Series([], dtype=pl.Utf8)\n})\n\n2query = \"excel\"\n\nstart_url = \"https://hh.ru/search/vacancy?experience=between1And3\" \\\n    \"&order_by=publication_time&ored_clusters=true\" \\\n    f\"&schedule=remote&text={query}&search_period=7\"\n\n3url = start_url\n\n4  headers = {\n      \"User-Agent\": \"Mozilla/5.0 (Windows NT 11.0; Win64; x64)\" \\\n        \"AppleWebKit/538.33 (KHTML, like Gecko) Chrome/98.0.4472.124 Safari/537.36\",\n      \"Accept-Language\": \"ru-RU,ru;q=0.9\",\n      \"Accept-Encoding\": \"gzip, deflate, br\",\n      \"Connection\": \"keep-alive\",\n      \"Upgrade-Insecure-Requests\": \"1\"\n  }\n\n5  keywords = [\"BPMN\", \"Jira\"]\n\n\n1\n\ncreate an empty table, the found information about vacancies will be added to this table\n\n2\n\nstart page of the request, this line takes into account the specified filters on hh, that is, you can go to hh, specify the search criteria, select filters, and then copy the link and paste it into the script\n\n3\n\nthis link serves as the main one for the transition, to go to the second page you need to add page=page number to the link, I use the code url=f‚Äô{start_url}&page={p}‚Äô which has a page constant start_url, and concatenation depending on the iteration of the loop\n\n4\n\nthese settings are needed when requesting requests.get(), to simulate that the user makes a request for the page, when the site requests data, we provide him with data from this list\n\n5\n\nkeywords that the script will search for on the vacancy pages\n\n\n\n\n\n\nChecking the received page (job vacancy)\nWhen requesting a page, there are cases where an incomplete page is received, missing some data This function checks for the presence of data, and if it‚Äôs incomplete, it re-requests the page\n\ndef check_correctly(url):\n1     for i in range(9):\n2          response = requests.get(url, headers=headers)\n3          soup = BeautifulSoup(response.text, 'html.parser')\n\n4          header = soup.find('div', {'class': 'vacancy-title'})\n5          if header:\n6               visit_and_check(url, soup, response)\n               break\n\n\n1\n\nIf the data is incomplete, we repeat the request\n\n2\n\nThe request to retrieve data from the website, url - the link to the data being requested, headers - this is the user-agent created earlier\n\n3\n\nParse the received object (which now contains the ‚Äúroughly speaking‚Äù HTML page), which can be parsed. The advantage is that during testing, no request is made to the website\n\n4\n\nLook for the div class named vacancy-title\n\n5\n\nCheck the header variable (why? Read the header description) to confirm if we received the complete page\n\n6\n\nCall the function and pass url - the link, soup - the local HTML, response - for searching words across the entire page\n\n\n\n\n\n\nScanning the job vacancy\nThis function extracts the necessary data from the page\n\ndef visit_and_check(url, soup, response):\n1  global df\n\n2  fkeys = []\n\n3  for keyword in keywords:\n4      if keyword in response.text:\n5        fkeys.append(keyword)\n  # Finding tags (skills) at the bottom of the page\n6  skill_elements = soup.find_all('li', {'data-qa': 'skills-element'})\n  skills = [li.find('div', class_=re.compile(r'magritte-tag__label')).text for li in skill_elements]\n\n7  header = soup.find('div', {'class': 'vacancy-title'})\n8  title = header.find('h1').text\n\n9  salary_str = header.find('span', {'data-qa': 'vacancy-salary-compensation-type-net'})\n  if salary_str:\n    # Find the first number in a string using a regular expression\n      match = re.search(r'\\d+', salary_str.text.replace('\\xa0', ''))\n      if match:\n        salary = int(match.group())\n      else:\n        salary = None\n  else:\n    salary = None\n\n10  df = df.vstack(pl.DataFrame({\n    \"URL\": [url],\n    \"–í–∞–∫–∞–Ω—Å–∏—è\" : [title],\n    \"–ó–∞—Ä–ø–ª–∞—Ç–∞\" : [salary],\n    \"Keyword\": [', '.join(fkeys)],\n    \"Tags\": [', '.join(skills)]}))\n\n\n1\n\nThis line is needed because of namespaces. The function is called for each job vacancy, and the result of the extracted data needs to be saved in a table. (After the function exits, all objects created inside it are deleted). Therefore, the table is created outside the function, globally, and this line is necessary to access the global table\n\n2\n\nThis list will contain the words found on the page (the list of words is specified in the keywords variable)\n\n3\n\nThe search for the keys specified in keywords happens here, with each keyword being processed one at a time\n\n4\n\nThe search for the key (word) is performed on the response.text object, which is the text returned by the website request\n\n5\n\nIf a keyword is found on the page, it is added to the empty list fkeys (which was created earlier)\n\n6\n\nWe search for the necessary data in the local HTML, which we parsed. In this case, we are looking for tags in the bottom part of the page\n\n7\n\nExtract the header\n\n8\n\nGet the job title\n\n9\n\nGet the salary rate (specified salary) Note: Some vacancies do not specify a salary, and there are cases where a salary range from‚Ä¶to is provided. These cases are handled below:\n\n10\n\nSave the extracted data into the table\n\n\n\n\n\n\nPage navigation and link search\nThis loop scans the main pages (where job vacancies are located), depending on the iteration, it navigates through pages 2, 3, 4‚Ä¶ on the website\n\n1for p in range(9):\n    if p &gt; 0:\n2        url = f'{start_url}&page={p}'\n        print(f\"--- Page {p} --- --- ---\")\n\n    # Searching for all links inside h2 tags\n3    for i in range(9):\n4        time.sleep(3)\n        print(f\"--- --- --- Iteration {i}\")\n5        response = requests.get(url, headers=headers)\n6        soup = BeautifulSoup(response.text, 'html.parser')\n7        if soup.find_all('h2'):\n            for h2 in soup.find_all('h2'):\n                for a in h2.find_all('a', href=True):\n                    absolute_url = requests.compat.urljoin(url, a['href'])\n8                    check_correctly(absolute_url)\n9            break\n\n\n1\n\nSpecify the number of main pages (where job vacancies are located) to scan\n\n2\n\nIf this is not the first iteration of the loop, add ‚Äúpage‚Äù and the page number (the current iteration number) to the URL\n\n3\n\nSometimes hh.ru returns an incomplete page, so in this loop, we check the validity of the received page. If it‚Äôs incomplete, we repeat the request until a complete page is obtained or until the number in the range() function is reached\n\n4\n\nBefore each new request, wait a little to avoid DDOSing the website. The goal is to gather data, simulating a simple navigation\n\n5\n\nThe request to retrieve data from the website, url - the link to the data being requested, headers - this is the user-agent that was created earlier\n\n6\n\nParse the received object (which now contains the ‚Äúroughly speaking‚Äù HTML page), which can be parsed. The advantage is that during testing, no request is made to the website\n\n7\n\nCheck the received page. If it‚Äôs incomplete, make another request (proceed to the next iteration)\n\n8\n\nPass the job vacancy link to the checking variable\n\n9\n\nAt this stage, when we reach if soup.find_all(‚Äòh2‚Äô), it means we have obtained the full link to the page, so no new request is needed for the same link\n\n\n\n\n\n\nSave\nSave the result to an xlsx file\n1wb = xlsxwriter.Workbook('Output.xlsx')\n2ws = wb.add_worksheet('DF')\n\n3df.write_excel(\n    workbook=wb,\n    worksheet='DF',\n    position=\"A1\",\n    table_style=\"Table Style Medium 3\",\n    dtype_formats={pl.Date: \"mm/dd/yyyy\"},\n    column_totals={\"score\": \"average\"},\n    float_precision=1,\n    autofit=True,\n)\n\n\n4ws.set_column('A:A', 10)\nws.set_column('B:B', 40)\nws.set_column('C:C', 10)\nws.set_column('D:D', 20)\nws.set_column('E:E', 90)\n\n5wb.close()\n\n1\n\nCreate a workbook object\n\n2\n\nCreate a sheet\n\n3\n\nInsert the table into the xlsx sheet and specify the column formats\n\n4\n\nSet the width of the columns\n\n5\n\nClose the xlsx file\n\n\n\n\nResult\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/portfolio/posts/OCR-tool.html#get-img-from-clipboard",
    "href": "content/portfolio/posts/OCR-tool.html#get-img-from-clipboard",
    "title": "OCR Reader",
    "section": "Get img from clipboard",
    "text": "Get img from clipboard\nFunction for get screenshot from clipboard\ndef get_image_from_clipboard():\n1    result = subprocess.run(['xclip', '-selection', 'clipboard', '-t', 'image/png', '-o'], stdout=subprocess.PIPE)\n\n2    if result.returncode != 0:\n        raise Exception(\"Can't get screenshot\")\n\n    return io.BytesIO(result.stdout)\n\n1\n\nGet screenshot from clipboard\n\n2\n\nException"
  },
  {
    "objectID": "content/portfolio/posts/OCR-tool.html#copy-result-into-clipboard",
    "href": "content/portfolio/posts/OCR-tool.html#copy-result-into-clipboard",
    "title": "OCR Reader",
    "section": "Copy result into clipboard",
    "text": "Copy result into clipboard\nThe func for copy OCR result in clipboard\ndef copy_text_to_clipboard(text):\n    subprocess.run(['xclip', '-selection', 'clipboard'], input=text.encode(), check=True)"
  },
  {
    "objectID": "content/portfolio/posts/revenue-segment.html",
    "href": "content/portfolio/posts/revenue-segment.html",
    "title": "Revenue segment",
    "section": "",
    "text": "In the R language ecosystem, there is a package WebR which allows creating a pie chart like this\n\n\n\n\n\n\n\n\n\n\n\n\nPlotly is a universal framework, available both in Python and R. It allows you to create an interactive pie chart in just a couple of lines of code."
  },
  {
    "objectID": "content/portfolio/posts/revenue-segment.html#r",
    "href": "content/portfolio/posts/revenue-segment.html#r",
    "title": "Revenue segment",
    "section": "",
    "text": "In the R language ecosystem, there is a package WebR which allows creating a pie chart like this"
  },
  {
    "objectID": "content/portfolio/posts/revenue-segment.html#python",
    "href": "content/portfolio/posts/revenue-segment.html#python",
    "title": "Revenue segment",
    "section": "",
    "text": "Plotly is a universal framework, available both in Python and R. It allows you to create an interactive pie chart in just a couple of lines of code."
  },
  {
    "objectID": "content/portfolio/index.html",
    "href": "content/portfolio/index.html",
    "title": "Portfolio",
    "section": "",
    "text": "The page contains visual and text examples of interesting results.\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nWeb Scraping\n\n\n12 min\n\n\n\nPython\n\n\nWeb-Scraping\n\n\n\n\n20 July 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSales Report\n\n\n5 min\n\n\n\nR\n\n\nGraphs\n\n\n\n\n01 July 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRevenue segment\n\n\n1 min\n\n\n\nR\n\n\nPython\n\n\nGraphs\n\n\n\n\n03 July 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOCR Reader\n\n\n2 min\n\n\n\nPython\n\n\nOCR\n\n\nEasyOCR\n\n\n\n\n06 December 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCheese sales\n\n\n1 min\n\n\n\nR\n\n\nGraphs\n\n\n\n\n06 July 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBinol vs Olive\n\n\n4 min\n\n\n\nPython\n\n\nSQLite\n\n\n\n\n14 December 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBeautiful export to Excel (xlsx)\n\n\n4 min\n\n\n\nR\n\n\nxlsx\n\n\nExcel\n\n\n\n\n10 July 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Shevchenko Vladimir Vladimirovich",
    "section": "",
    "text": "Translate üí¨\nüá¨üáß ¬¶ üá≥üá¥ ¬¶ üá∑üá∫\n\n\n\n\n\n\n\n\nDate of Birth\n08/29/1997 (27 years old)\n\n\nGender\nMale\n\n\nMarital Status\nSingle\n\n\nEducation\nBachelor Degree\n\n\nLocation\nBj√∏rnafjordenm, Norway\n\n\nJob Search Status\nOpen to offers"
  },
  {
    "objectID": "about.html#section",
    "href": "about.html#section",
    "title": "Shevchenko Vladimir Vladimirovich",
    "section": "2016 ‚Äì 2020",
    "text": "2016 ‚Äì 2020\n\n\n\n\n\n\n\nEducational Institution\nDmytro Motornyi Tavria StateAgrotechnological University\n\n\nProgram Subject Area\nComputer sciences and information technologies\n\n\nQualification\nBachelor in Computer Sciences and Information Technologies\n\n\nForm of Training\nFull-Time"
  },
  {
    "objectID": "about.html#data-processing-and-reporting-tools",
    "href": "about.html#data-processing-and-reporting-tools",
    "title": "Shevchenko Vladimir Vladimirovich",
    "section": "Data Processing and Reporting Tools",
    "text": "Data Processing and Reporting Tools\n\n\n\nTools\nAreas of Application\n\n\n\n\nExcel\nFormulas, Pivot Tables\n\n\nPowerPoint\nIn-house Presentation Creation\n\n\nPython\npolars, pandas, seaborn, plotly, etc.\n\n\nR\ndplyr, ggplot, futureverse, glue, etc.\n\n\nSQL\nOracle, SQLite\n\n\nQuarto / RMarkdown\nReporting"
  },
  {
    "objectID": "about.html#programming",
    "href": "about.html#programming",
    "title": "Shevchenko Vladimir Vladimirovich",
    "section": "Programming",
    "text": "Programming\n\n\n\nGo\ncolly, go-telegram\n\n\nDocker\nuse of templates containers"
  },
  {
    "objectID": "about.html#os-and-ide",
    "href": "about.html#os-and-ide",
    "title": "Shevchenko Vladimir Vladimirovich",
    "section": "OS and IDE",
    "text": "OS and IDE\nPreferred Tools\n\n\n\nLinux\nEmacs\nDBeaver"
  },
  {
    "objectID": "about.html#june-2023---december-2023",
    "href": "about.html#june-2023---december-2023",
    "title": "Shevchenko Vladimir Vladimirovich",
    "section": "June 2023 - December 2023",
    "text": "June 2023 - December 2023\n\n\n\nJOB TITLE\nEconomist\n\n\n\n\nORGANIZATION\nMelitopol Dairy Plant LLC\n\n\nEXPERIENCE WORK\n~7 months\n\n\n\nJOB DESCRIPTION AND RESPONSIBILITIES:\n\nPreparing and supporting reports for three enterprises: Melitopol Dairy Plant, Cheeses of Tavria, Dolphin  (data collection, processing, analysis, and visualization)\n\nOversight of product and material stock warehouses\n\nSales analysis by stores\n\nProduct cost calculation"
  },
  {
    "objectID": "about.html#november-2020---october-2022",
    "href": "about.html#november-2020---october-2022",
    "title": "Shevchenko Vladimir Vladimirovich",
    "section": "November 2020 - October 2022",
    "text": "November 2020 - October 2022\n\n\n\n\n\n\n\nJOB TITLE\nChief Specialist of theRisk Management Department\n\n\n\n\nORGANIZATION\nForward Bank LLC\n\n\nEXPERIENCE WORK\n~2 years\n\n\n\nJOB DESCRIPTION AND RESPONSIBILITIES:\n\nGeneration of reports, pass rates, and risk indicators\n\nCustomer checks against blacklists, personal data, BKI, PTI\n\nWriting and optimizing audit scripts; developing new risk indicators\n\nManaging Oracle database tables (creation, deletion, modification of a table‚Äôs structure)\n\nData cleaning\nInformation processing (segmentation, aggregation, filtering, visualization)"
  },
  {
    "objectID": "about.html#section-1",
    "href": "about.html#section-1",
    "title": "Shevchenko Vladimir Vladimirovich",
    "section": "2016 - 2018",
    "text": "2016 - 2018\n\n\n\nJOB TITLE\nTech Specialist\n\n\n\n\nORGANIZATION\nBand\n\n\nEXPERIENCE WORK\n~2 years\n\n\n\nJOB DESCRIPTION AND RESPONSIBILITIES:\n\nRemote technical support for employees\n\nInstallation and configuration of programs for Windows/Linux\n\nVPS (VDS) rental and support\n\nWorkflow automation"
  },
  {
    "objectID": "about-ru.html",
    "href": "about-ru.html",
    "title": "–®–µ–≤—á–µ–Ω–∫–æ –í–ª–∞–¥–∏–º–∏—Ä –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á",
    "section": "",
    "text": "–ü–µ—Ä–µ–≤–æ–¥ üí¨\nüá¨üáß ¬¶ üá≥üá¥ ¬¶ üá∑üá∫\n\n\n\n\n\n\n\n\n–î–ê–¢–ê –†–û–ñ–î–ï–ù–ò–Ø\n29.08.1997 –≥. (27 –ª–µ—Ç)\n\n\n–ü–û–õ\n–ú—É–∂—Å–∫–æ–π\n\n\n–°–ï–ú–ï–ô–ù–û–ï –ü–û–õ–û–ñ–ï–ù–ò–ï\n–•–æ–ª–æ—Å—Ç\n\n\n–û–ë–†–ê–ó–û–í–ê–ù–ò–ï\n–í—ã—Å—à–µ–µ\n\n\n–ú–ï–°–¢–û–ù–ê–•–û–ñ–î–ï–ù–ò–ï\nBj√∏rnafjordenm, –ù–æ—Ä–≤–µ–≥–∏—è\n\n\n–ü–û–ò–°–ö –†–ê–ë–û–¢–´\n–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"
  },
  {
    "objectID": "about-ru.html#section",
    "href": "about-ru.html#section",
    "title": "–®–µ–≤—á–µ–Ω–∫–æ –í–ª–∞–¥–∏–º–∏—Ä –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á",
    "section": "2016 ‚Äì 2020",
    "text": "2016 ‚Äì 2020\n\n\n\n\n\n\n\n–£–ß–ï–ë–ù–û–ï –ó–ê–í–ï–î–ï–ù–ò–ï\n–¢–∞–≤—Ä–∏—á–µ—Å–∫–∏–π –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π –ê–≥—Ä–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç\n\n\n–°–ü–ï–¶–ò–ê–õ–¨–ù–û–°–¢–¨\nIT\n\n\n–§–ê–ö–£–õ–¨–¢–ï–¢\n–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ –Ω–∞—É–∫–∏\n\n\n–§–û–†–ú–ê –û–ë–£–ß–ï–ù–ò–Ø\n–û—á–Ω–∞—è"
  },
  {
    "objectID": "about-ru.html#–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ",
    "href": "about-ru.html#–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ",
    "title": "–®–µ–≤—á–µ–Ω–∫–æ –í–ª–∞–¥–∏–º–∏—Ä –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á",
    "section": "–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ",
    "text": "–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ\n\n\n\nGo\ncolly, go-telegram\n\n\nDocker\n–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —à–∞–±–ª–æ–Ω–æ–≤"
  },
  {
    "objectID": "about-ru.html#os-–∏-ide",
    "href": "about-ru.html#os-–∏-ide",
    "title": "–®–µ–≤—á–µ–Ω–∫–æ –í–ª–∞–¥–∏–º–∏—Ä –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á",
    "section": "OS –∏ IDE",
    "text": "OS –∏ IDE\n–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã\n\n\n\nLinux\nEmacs\nDbeaver"
  },
  {
    "objectID": "about-ru.html#–∏—é–Ω—å-2023---–¥–µ–∫–∞–±—Ä—å-2023",
    "href": "about-ru.html#–∏—é–Ω—å-2023---–¥–µ–∫–∞–±—Ä—å-2023",
    "title": "–®–µ–≤—á–µ–Ω–∫–æ –í–ª–∞–¥–∏–º–∏—Ä –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á",
    "section": "–ò—é–Ω—å 2023 - –î–µ–∫–∞–±—Ä—å 2023",
    "text": "–ò—é–Ω—å 2023 - –î–µ–∫–∞–±—Ä—å 2023\n\n\n\n–î–û–õ–ñ–ù–û–°–¢–¨\n–≠–∫–æ–Ω–æ–º–∏—Å—Ç\n\n\n\n\n–û–†–ì–ê–ù–ò–ó–ê–¶–ò–Ø\nOOO ¬´–ú–µ–ª–∏—Ç–æ–ø–æ–ª—å—Å–∫–∏–π –º–æ–ª–æ–∫–æ–∑–∞–≤–æ–¥¬ª\n\n\n–û–ü–´–¢ –†–ê–ë–û–¢–´\n~7 –º–µ—Å\n\n\n\n–î–û–õ–ñ–ù–û–°–¢–ù–´–ï –û–ë–Ø–ó–ê–ù–ù–û–°–¢–ò:\n\n–°–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –æ—Ç—á–µ—Ç–æ–≤ –ø–æ 3–º –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è–º: –ú–µ–ª–∏—Ç–æ–ø–æ–ª—å—Å–∫–∏–π –º–æ–ª–æ–∫–æ–∑–∞–≤–æ–¥, –°—ã—Ä—ã –¢–∞–≤—Ä–∏–∏, –î–µ–ª—å—Ñ–∏–Ω (—Å–±–æ—Ä, –æ–±—Ä–∞–±–æ—Ç–∫–∞, –∞–Ω–∞–ª–∏–∑ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö)\n–ö–æ–Ω—Ç—Ä–æ–ª—å –∑–∞ —Å–∫–ª–∞–¥–æ–º –ø—Ä–æ–¥—É–∫—Ü–∏–∏ –∏ —Å–∫–ª–∞–¥–æ–º –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∞—Å–æ–≤\n–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–¥–∞–∂ –ø–æ –º–∞–≥–∞–∑–∏–Ω–∞–º\n–ü—Ä–æ—Å—á–µ—Ç —Å–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤"
  },
  {
    "objectID": "about-ru.html#–Ω–æ—è–±—Ä—å-2020---–æ–∫—Ç—è–±—Ä—å-2022",
    "href": "about-ru.html#–Ω–æ—è–±—Ä—å-2020---–æ–∫—Ç—è–±—Ä—å-2022",
    "title": "–®–µ–≤—á–µ–Ω–∫–æ –í–ª–∞–¥–∏–º–∏—Ä –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á",
    "section": "–ù–æ—è–±—Ä—å 2020 - –û–∫—Ç—è–±—Ä—å 2022",
    "text": "–ù–æ—è–±—Ä—å 2020 - –û–∫—Ç—è–±—Ä—å 2022\n\n\n\n–î–û–õ–ñ–ù–û–°–¢–¨\n–ì–ª–∞–≤–Ω—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞\n\n\n\n\n–û–†–ì–ê–ù–ò–ó–ê–¶–ò–Ø\nOOO ¬´Forward Bank¬ª\n\n\n–û–ü–´–¢ –†–ê–ë–û–¢–´\n~ 2 –≥–æ–¥–∞\n\n\n\n–î–û–õ–ñ–ù–û–°–¢–ù–´–ï –û–ë–Ø–ó–ê–ù–ù–û–°–¢–ò:\n\n–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç—á–µ—Ç–æ–≤ –Ω–æ—Ä–º—ã –ø—Ä–æ–ø—É—Å–∫–∞ –∏ —Ä–∏—Å–∫–æ–≤\n–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ —á—ë—Ä–Ω–æ–º—É —Å–ø–∏—Å–∫—É, –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, BKI, PTI\n–ù–∞–ø–∏—Å–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–æ–≤ –ø—Ä–æ–≤–µ—Ä–æ–∫, —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤—ã—Ö —Ä–∏—Å–∫–æ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π\n–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –≤ –±–∞–∑–µ Oracle (—Å–æ–∑–¥–∞–Ω–∏–µ, —É–¥–∞–ª–µ–Ω–∏–µ, –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ, –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö‚Ä¶)\n–ü—Ä–∏–≤–∏–¥–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ ¬´—á–∏—Å—Ç—ã–π –≤–∏–¥¬ª\n–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (—Ä–∞–∑—Ä–µ–∑—ã, –∞–≥—Ä–µ–≥–∞—Ü–∏—è, —Ñ–∏–ª—å—Ç—Ä—ã‚Ä¶)"
  },
  {
    "objectID": "about-ru.html#section-1",
    "href": "about-ru.html#section-1",
    "title": "–®–µ–≤—á–µ–Ω–∫–æ –í–ª–∞–¥–∏–º–∏—Ä –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á",
    "section": "2016 - 2018",
    "text": "2016 - 2018\n\n\n\n–î–û–õ–ñ–ù–û–°–¢–¨\n–¢–µ—Ö –ü–æ–¥–¥–µ—Ä–∂–∫–∞\n\n\n\n\n–û–†–ì–ê–ù–ò–ó–ê–¶–ò–Ø\n–ß–ü ¬´Band¬ª\n\n\n–û–ü–´–¢ –†–ê–ë–û–¢–´\n~ 2 –≥–æ–¥–∞\n\n\n\n–î–û–õ–ñ–ù–û–°–¢–ù–´–ï –û–ë–Ø–ó–ê–ù–ù–û–°–¢–ò:\n\n–£–¥–∞–ª–µ–Ω–Ω–∞—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤\n–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º –¥–ª—è Windows, Linux\n–ê—Ä–µ–Ω–¥–∞ –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ VPS (VDS)\n–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤"
  },
  {
    "objectID": "content/portfolio/posts/sales_report.html",
    "href": "content/portfolio/posts/sales_report.html",
    "title": "Sales Report",
    "section": "",
    "text": "Binol stores with an area of up to 15m¬≤\nThe presented chart shows the month-by-month sales dynamics. The data indicates that sales volumes in such outlets experience significant fluctuations throughout the year.\n\n\n\n\n\n\nNote\n\n\n\nData for the store 57/9 Kirova Street is unavailable due to its closure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinol stores with an area of 15m¬≤ or more\n\n\n\n\n\n\n\n\n\n\n\n\nTotal by months\nThe total revenue from the sale of Binol and Olive flowers.\n\n\n\n\n\n\nNote\n\n\n\nThe revenue from Olive was deducted from the total sales of the stores. That is, the Binol figures also include third-party products (warehouse goods).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/portfolio/posts/cheese-sales.html",
    "href": "content/portfolio/posts/cheese-sales.html",
    "title": "Cheese sales",
    "section": "",
    "text": "Example of a 3D Pie Chart using the plotrix library\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/portfolio/posts/sales-binol-olive.html",
    "href": "content/portfolio/posts/sales-binol-olive.html",
    "title": "Binol vs Olive",
    "section": "",
    "text": "Binol and Olive is concurire market for buys a flowers\n\nLibrary\n\nfrom sqlalchemy import create_engine\nimport polars as pl\n\nfrom matplotlib.ticker import FuncFormatter\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom locale import setlocale, LC_TIME\n\n\n\nGet data asdasdasd\n\ncon = create_engine(f\"sqlite:////home/eno/Documents/db/portfolio.db\")\n\ndf = pl.read_database(\n    \"select * from salles_binol_olivia where Shops not in('Main store', 'Victoria 38/4')\", con.connect()\n).with_columns(\n    pl.col('DT').str.to_date(format='%Y-%m-%d')\n)\n\n\n\nAgregate table\n\nsns.set_theme()\n\ndf_melted = df.melt(id_vars=[\"DT\", \"Shops\"], value_vars=[\"Binol\", \"Olive\"],\n                    variable_name=\"Product\", value_name=\"Value\")\n\ng = sns.FacetGrid(df_melted,\n                  col='Shops',\n                  hue='Product',        # coloring with a new color\n                  col_wrap=6,           # number of plots displayed in one row\n                  sharey=False,         # y-axis shared across all plots\n                  sharex=False,         # x-axis shared across all plots\n                  height=4)             # height\n\ng = g.map(sns.lineplot, 'DT', 'Value', marker='o', markersize=4)\n\n# Apply formatting function to the Y-axis of each subplot\nfor ax in g.axes.flat:\n    # Increase the maximum Y-axis value by 10%\n    ylim = ax.get_ylim()\n    ax.set_ylim((ylim[0], ylim[1] * 1.1))\n\n    # Draw trend lines for each line in ax.lines\n    for line in ax.lines:\n        sns.regplot(\n            x=line.get_xdata(), y=line.get_ydata(), ax=ax,\n            scatter=False,         # No markers (only trend line)\n            color='gray',          # Set line color to gray\n            ci=None,               # No confidence interval shading\n            line_kws={'linestyle': '--'}  # Dashed line\n        )\n    # Format X-axis labels\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n\n# Text annotation function\ndef f(x, y, **kwargs):\n    ax = plt.gca()  # Get current axis\n    for i in range(len(x)):\n        ax.annotate(f\"{y.values[i]:.1f}\", xy=(x.values[i], y.values[i]), fontsize=8,\n                    xytext=(0, 10), textcoords=\"offset points\",\n                    color=\"black\",  # Set color same as line color\n                    bbox=dict(boxstyle=\"round\", ec=\"none\", fc=\"gray\", alpha=0.3, pad=0.3),\n                    va=\"center\", ha=\"center\")\n\ng.map(f, 'DT', 'Value')\n\ng.fig.subplots_adjust(top=.9)         # space for the suptitle from the plot\ng.fig.suptitle('Salles')              # title of the plot\n\ng.add_legend(title=\"Product\")\nsns.move_legend(\n    g, \"center\", bbox_to_anchor=(.5, 1), ncol=5, title=None, frameon=False,\n)\n\n# specify X/Y axis labels\ng.set_axis_labels(\"Date\", \"Amount\")\n\n# set the title of each subplot based on the data table\ng.set_titles(\"{col_name}\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/portfolio/posts/export-to-xlsx.html",
    "href": "content/portfolio/posts/export-to-xlsx.html",
    "title": "Beautiful export to Excel (xlsx)",
    "section": "",
    "text": "Note\n\n\n\nThis article briefly describes the functions of the openxlsx package. Below is an example of the interaction of functions and the final result of exporting a table from R ‚Üí xlsx\n\n\n\nLibraryes and styles\n\nlibrary(openxlsx)\n\n## --- Colors\ngray   &lt;- createStyle(fgFill = '#d9d9d9')\ngreen  &lt;- createStyle(fgFill = '#c6e0b4')\nred    &lt;- createStyle(fgFill = '#f9a1a1')\nblue   &lt;- createStyle(fgFill = '#bdd7ee')\nyellow &lt;- createStyle(fgFill = '#ffe699')\nros    &lt;- createStyle(fgFill = '#fce4d6')\nborder &lt;- createStyle(fgFill = '#333333')\n\n## --- Styles\nst_bord &lt;- createStyle(numFmt = \"#,##0\",\n                       border = 'TopBottomLeftRight',borderColour = '#cccccc')\nst_head &lt;- createStyle(textDecoration = \"bold\", halign = \"center\",\n                       border = 'TopBottomLeftRight',borderColour = border)\nst_bot  &lt;- createStyle(textDecoration = 'bold',border = \"top\",\n                       borderColour = border,borderStyle = \"medium\")\nst_name &lt;- createStyle(halign = \"center\", textDecoration = 'bold',\n                       fontSize = 16, border = 'TopBottomLeftRight',\n                       borderColour = border, borderStyle = 'medium')\nst_bold &lt;- createStyle(textDecoration = 'bold',border = \"left\",\n                       borderColour = border,borderStyle = \"medium\")\n\n\n\nWorkbook and Options\nWe create a workbook (an object in RAM) and a sheet in this workbook named Sales, to which we apply several options: - orientation :: portrait - specifies the paper orientation (an option for printing on paper) - pageSetup :: parameters for page margin (helps fit more information on the page when printing on paper)\n\n## Create a book\nwb &lt;- createWorkbook()\n\n## Adding a Sheet to the xlsx Workbook\naddWorksheet(wb, sheetName = \"Sales\", orientation = 'portrait', gridLines = FALSE)\n\n## Setting Margins\npageSetup(wb, \"Sales\", left = 0.25, top = 0.25, right = 0.25,bottom = 0.25)\n\n\n\nWriting and Customization\n\nmergeCells - merges cells (the range of merged cells is determined automatically) cols = 1:length(fin_sales)\naddStyle - applies the ‚Äúst_name‚Äù style to the specified range\nsetColWidths - sets the width of the column\n\n\n## Writing Data to the xlsx Sheet\nwriteData(wb, 'Sales', \"Sales\")\n\n## Merging Cells\nmergeCells(wb, sheet = \"Sales\", cols = 1:length(fin_sales), rows = 1)\n\n## Apply the st_name Style to the First Row from the First Column to the Length of the fin_sales Table\naddStyle(wb, 'Sales', st_name, 1, 1:length(fin_sales), stack = TRUE)\n\nwriteData(wb, 'Sales', fin_sales, startRow = 3)\naddStyle(wb, 'Sales', st_head, 3, 1:length(fin_sales),\n         gridExpand = TRUE, stack = TRUE)\n\n## Set Column Widths\nsetColWidths(wb, \"Sales\", cols = 1, widths = 25)\n\naddStyle(wb, 'Sales', st_bord, 4:(nrow(fin_sales)+3),\n         1:length(fin_sales), stack = TRUE, gridExpand = TRUE)\n\n## Apply the st_bot Style to Cells in the Column with Data\naddStyle(wb, 'Sales', st_bot, (which(!is.na(temp_sales$PROC))+3),\n         1:length(temp_sales), stack = TRUE, gridExpand = TRUE)\n\naddStyle(wb, 'Sales', st_bot, nrow(fin_sales)+3, 1:length(fin_sales), stack = TRUE)\naddStyle(wb, 'Sales', createStyle(halign = \"center\"),\n         3:(nrow(fin_sales)+3), length(fin_sales), stack = TRUE, gridExpand = TRUE)\nsetColWidths(wb, \"Sales\", cols = length(fin_sales), widths = 18)\naddStyle(wb, 'Sales', gray, 3, 1, stack = TRUE)\naddStyle(wb, 'Sales', ros, 3, c(2,3), stack = TRUE, gridExpand = TRUE)\naddStyle(wb, 'Sales', green, 3, 4, stack = TRUE)\naddStyle(wb, 'Sales', blue, 3, 5, stack = TRUE)\naddStyle(wb, 'Sales', yellow, 3, 6, stack = TRUE)\n\n\n\nResult\n\n\n\nConclusion\nThis method of saving to xlsx is, on one hand, ‚Äúverbose,‚Äù but on the other hand, it is very flexible in customization, allowing you to write/apply styles down to the individual cell.\nIf a similar report is generated regularly, where only the data is updated, this approach makes sense. In the long term, it will pay off in subsequent iterations of report generation.\n\n\n\n\n Back to top"
  }
]